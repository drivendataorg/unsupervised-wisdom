{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "-gECjeVBI5m9",
   "metadata": {
    "id": "-gECjeVBI5m9"
   },
   "source": [
    "<h1>Falls in seniors: predicting their health outcomes using narratives and baseline demographic data</h1>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "**Goals & Rationales**\n",
    "\n",
    "Timely admission to the emergency department (ED) is an important determinant of outcomes in elderly patients. Yet, unnecessary hospital visits increase the burden on health systems and risk bringing anxiety and stress to patients and their family members and caregivers. Accordingly, we examined:\n",
    "- (Ia) how the time till hospital visit (nicknamed as “delay time”) as currently captured in the narratives may be predictive of patient outcomes;\n",
    "- (Ib) Once “delay times” were extracted, we also developed and validate prognostic survival models that predict time to an adverse outcome using data collected at baseline (narrative, baseline, and delay times).\n",
    "\n",
    "**Methods**\n",
    "\n",
    "We employed data from both primary and supplementary as shown in [Table below](#table). Our evaluation set consists of cases from 2013-2020. Note that these subsets yield sizes comparable to open survival datasets reported in recent benchmarks, e.g. SUPPORT (n=9,105), FLCHAIN (n=7,894).\n",
    "\n",
    "As problem statement instructs, we removed parts of each narrative that were already captured in the other columns, i.e. starting characters on sex, age, and end phrases that follow the marker ```DX``` (or equivalent markers such as ```***```, ```>>```). Then, we performed preprocessing steps on the narratives. Next, we computed word embeddings to describe the textural data.\n",
    "\n",
    "**Derivations of survival times**\n",
    "\n",
    "We inferred the number of hours from the time of fall incident to the time of hospital visit by searching for keywords, such as “1 DAY AGO”, “YESTERDAY”, “LAST NITE”. Cases whose survival times cannot be determined were excluded in the survival analyses. Patients whose dispositions do not match the outcome definitions were treated as censored (these include patients who left the hospital before being seen).\n",
    "\n",
    "**Model development**\n",
    "\n",
    "We performed Bayesian optimization (```Optuna``` package) to find the optimal parameters of eXtreme Gradient Boosters (XGB) survival models that predict $P$ probabilities of experiencing an adverse outcome at times 1 to $P$. For baseline comparison, we also fitted regularized Cox’s regression models. For each outcome, we developed two survival models (XGB/Cox) under different combinations of three input types:\n",
    "1. patient’s baseline data\n",
    "1. raw word embeddings\n",
    "1. their dimensionality reduced versions.\n",
    "\n",
    "**Key messages**\n",
    "\n",
    "Based on our key findings as analyzed below, we recommend that “delay time” to hospital be tracked, as our analyses showed that this additional information can be used to predict adversity in patient outcomes, with predictive accuracy reaching 0.72 in C-index. We note that “delay time” is different from the hour of the fall (morning/evening), which is also not tracked in the current NEISS coding manual but has long been suggested in literature (e.g. [SPLATT mnemonic](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7905119/)).\n",
    "\n",
    "# Objectives of this notebook\n",
    "\n",
    "Audience will be able to...\n",
    "- Hands on experience of natural language processing (NLP) and following encoders/transformers   \n",
    "- Review the practical uses of the following packages/ tasks:\n",
    "    \n",
    "<a href id=\"table\"></a>\n",
    "\n",
    "\n",
    "| Tasks | Package used |\n",
    "|:--|:--|    \n",
    "| Bayesian optimization | ```optuna```  |\n",
    "| Efficient dataset querying | ```polars``` |\n",
    "| Dimensionality reduction & visualization |```UMAP``` |\n",
    "| Survival models | ```xgboost```, ```sksurv```, ```lifelines```|\n",
    "| Evaluation metrics for survival data analysis | ```sksurv.metrics.concordance_index_censored``` <br>```sksurv.metrics.brier_score```, ```scipy.stats.spearmanr``` |\n",
    "\n",
    "\n",
    "## Shortening runtime\n",
    "\n",
    "\n",
    "In order to shorten the runtime, we recommend only running the key results / trials, e.g.\n",
    "\n",
    "**Option A**: XGB Optuna loop will only try the following input settings\n",
    "```\n",
    "for mid in ['xgb',]:\n",
    "    for input_type in [25]: # rather than other combinations of inputs [19,21,22,23,24,26]\n",
    "```\n",
    "**Option B**: Run Cox regression only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f78e5-3655-4b1d-bf01-223157bb7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YtOoyyvHQrc-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtOoyyvHQrc-",
    "outputId": "788ad19a-3b48-4525-8e54-28204130b9a1"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    # tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(1119)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NvUtkv-wWh25",
   "metadata": {
    "id": "NvUtkv-wWh25"
   },
   "source": [
    "# Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YqsPbbY2Wji3",
   "metadata": {
    "id": "YqsPbbY2Wji3"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f35894-41d1-47c4-9df2-a947abba1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path.cwd().parent / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bz5c8TZDWjTA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bz5c8TZDWjTA",
    "outputId": "ca1dd074-cffe-466f-c374-13d6fdca96ac"
   },
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    with Path(folder / \"variable_mapping.json\").open(\"r\") as f:\n",
    "        mapping = json.load(f, parse_int=True)\n",
    "\n",
    "    for c in mapping.keys():\n",
    "        mapping[c] = {int(k): v for k, v in mapping[c].items()}\n",
    "\n",
    "    df = pd.read_csv(folder / \"primary_data.csv\", parse_dates=['treatment_date'],\n",
    "                   dtype={\"body_part_2\": \"Int64\", \"diagnosis_2\": \"Int64\", 'other_diagnosis_2': 'string'} )\n",
    "\n",
    "    df2 = pd.read_csv(folder / \"supplementary_data.csv\",  parse_dates=['treatment_date'],\n",
    "                    dtype={\"body_part_2\": \"Int64\", \"diagnosis_2\": \"Int64\", 'other_diagnosis_2': 'string' } )\n",
    "\n",
    "    org_columns = df2.columns\n",
    "    df['source']=1\n",
    "    df2['source']=2\n",
    "\n",
    "    merged_df = pd.concat( (df, df2)) #, left_on='cpsc_case_number', right_on='cpsc_case_number', how='outer' )\n",
    "    merged_df.drop_duplicates( inplace=True )\n",
    "    merged_df.reset_index( inplace=True )\n",
    "\n",
    "    primary_cid = merged_df.iloc[ np.where( merged_df.source==1)[0],: ].cpsc_case_number\n",
    "    supp_cid    = merged_df.iloc[ np.where( merged_df.source==2)[0],: ].cpsc_case_number\n",
    "\n",
    "    trn_case_nums = np.intersect1d( primary_cid, supp_cid )\n",
    "    tst_case_nums = np.setdiff1d( supp_cid, primary_cid )\n",
    "\n",
    "    print( 'Trn:Tst ratio:', len(trn_case_nums)/ len(tst_case_nums),  )\n",
    "\n",
    "    def add_cols( df2 ):\n",
    "\n",
    "        df2['month'] = df2.treatment_date.dt.month\n",
    "        df2['year'] = df2.treatment_date.dt.year\n",
    "\n",
    "        '''\n",
    "        0. Not stated\n",
    "        1. White: A person having origins in any of the Europe, Middle East, or North Africa.\n",
    "        2. Black/African American: A person having origins in any of the black racial groups of Africa.\n",
    "        3. ED record indicates more than one race (e.g., multiracial, biracial)\n",
    "        4. Asian: A person having origins in any of the original peoples of the Far East, Southeast Asia, or the Indian subcontinent\n",
    "        5. American Indian/Alaska Native: A person having origins in any of the original peoples of North and South America (including Central America), and who maintains tribal affiliation or community attachment.\n",
    "        6. Native Hawaiian/Pacific Islander: A person having origins in any of the original peoples of Hawaii, Guam, Samoa, or other Pacific Islands.\n",
    "        7. White Hispanic 1 Race=1\n",
    "        8. Black Hispanic 1 Race=2\n",
    "        '''\n",
    "\n",
    "        k = 'race_white'\n",
    "        df2[k] = 0 # non-white\n",
    "        q=np.where( df2['race'] == 0 )[0]\n",
    "        df2.loc[ q, k] = -1 # not stated\n",
    "        q=np.where( df2['race'] == 1 )[0]\n",
    "        df2.loc[ q, k] = 1\n",
    "\n",
    "        k = 'race_4'\n",
    "        df2[k] = 0 # non-white\n",
    "        q=np.where( df2['race'] == 0 )[0]\n",
    "        df2.loc[ q, k] = -1\n",
    "        q=np.where( df2['race'] == 4 )[0]\n",
    "        df2.loc[ q, k] = -2\n",
    "        q=np.where( df2['race'] == 1 )[0]\n",
    "        df2.loc[ q, k] = 1\n",
    "\n",
    "\n",
    "        df2['race_recoded'] = 0\n",
    "        df2['race_recoded'] = df2['race'].copy()\n",
    "        q=np.where( (df2['hispanic'] == 1 ) & (df2['race'] == 1) )[0]\n",
    "        df2.loc[q, 'race_recoded'] = 7\n",
    "        q=np.where( (df2['hispanic'] == 1 ) & (df2['race'] == 2) )[0]\n",
    "        df2.loc[q, 'race_recoded'] = 8\n",
    "\n",
    "        df2['severity'] = df2['disposition'].copy()\n",
    "        df2['severity'].replace( {9: np.nan, 6: 1, 5:2,  1:3,  2:4 ,  4:5,  8: 6 }, inplace=True)\n",
    "\n",
    "\n",
    "        df2['age_cate']= 0\n",
    "        df2['age_cate']= pd.cut(\n",
    "        df2.age,\n",
    "        bins=[0,65,75,85,95,150],\n",
    "        labels=[\"1: 65 or under\", \"2: 65-74\", \"3: 74-85\", \"4: 85-94\", \"5: 95+\"],\n",
    "        )\n",
    "\n",
    "        df2['age_cate'] = pd.Categorical(df2.age_cate).copy()\n",
    "        df2['age_cate_binned'] = df2.age_cate.cat.codes\n",
    "\n",
    "        # drop the 3 cases of unknown sex and then map codes to English words\n",
    "        df2 = df2[df2.sex != 0]\n",
    "        return df2\n",
    "\n",
    "    # add variables\n",
    "    merged_df = add_cols( merged_df )\n",
    "\n",
    "    for col in mapping.keys():\n",
    "        if col != 'disposition':\n",
    "            merged_df[col] = merged_df[col].map(mapping[col])\n",
    "\n",
    "    return merged_df, org_columns, trn_case_nums, tst_case_nums, mapping\n",
    "\n",
    "def load_decoded(folder):\n",
    "    decoded_df2=pd.read_csv(folder / 'decoded_df2_unique.csv')\n",
    "\n",
    "    decoded_df2.sex = (decoded_df2.sex == 'MALE').astype(int)\n",
    "    for k in [ 'alcohol','fire_involvement','drug', ]:\n",
    "        decoded_df2[k] = ( decoded_df2[k] == 'Yes').astype(int)\n",
    "    dic = {}\n",
    "    for k in [ 'location','product_1','product_2','product_3','body_part','body_part_2' ]:\n",
    "        dic[k] = {k: { i:l for l,i in enumerate( decoded_df2[k].unique() ) } }\n",
    "        decoded_df2.replace( dic[k], inplace=True )\n",
    "\n",
    "    def add_race_categories( df2 ):\n",
    "        df2.reset_index(inplace=True)\n",
    "        k = 'race_white'\n",
    "        df2[k] = 0 # non-white\n",
    "        q=np.where( df2['race'] == 'N.S.' )[0]\n",
    "        df2.loc[ q, k] = -1 # not stated\n",
    "        q=np.where( df2['race'] == 'WHITE' )[0]\n",
    "        df2.loc[ q, k] = 1\n",
    "\n",
    "        k = 'race_4'\n",
    "        df2[k] = 0 # non-white\n",
    "        q=np.where( df2['race'] == 'N.S.' )[0]; print(len(q))\n",
    "        df2.loc[ q, k] = -1\n",
    "        q=np.where( df2['race'] == 'ASIAN' )[0]; print(len(q))\n",
    "        df2.loc[ q, k] = -2\n",
    "        q=np.where( df2['race'] == 'WHITE' )[0]; print(len(q))\n",
    "        df2.loc[ q, k] = 1\n",
    "        return df2\n",
    "\n",
    "    decoded_df2 = add_race_categories(decoded_df2 )\n",
    "    print( 'race=white?', decoded_df2['race_white'].unique() ) #check\n",
    "\n",
    "    return decoded_df2\n",
    "\n",
    "if ('decoded_df2' in globals())==False:\n",
    "    _, org_columns, trn_case_nums, tst_case_nums, mapping = get_data(DATA_FOLDER)\n",
    "    decoded_df2 = load_decoded(DATA_FOLDER)\n",
    "\n",
    "att =['location','product_1','product_2','product_3','fire_involvement','body_part','drug','alcohol', 'sex', 'age_cate_binned','race_recoded','year','month']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ddx7fTxujMgR",
   "metadata": {
    "id": "Ddx7fTxujMgR"
   },
   "source": [
    "## Add day of week to the decoded dataframe ```decoded_df2```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MZsoMzM-bdaQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MZsoMzM-bdaQ",
    "outputId": "a1135ba0-0bc7-4462-9ed1-840ae1792122"
   },
   "outputs": [],
   "source": [
    "decoded_df2['weekday'] = pd.to_datetime( decoded_df2.treatment_date ).dt.weekday\n",
    "decoded_df2.head(1).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QXLkZkUpWAqr",
   "metadata": {
    "id": "QXLkZkUpWAqr"
   },
   "source": [
    "## Extract time-to-event (```time2event```) data from each narrative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZYuCe29WjNq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZYuCe29WjNq",
    "outputId": "1dc5cc7c-709d-48b1-d8b2-66c00e04de75"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import polars as pol\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import scipy\n",
    "from sksurv.metrics import *\n",
    "\n",
    "labels = ['3h','6h','9h','12h','15h','18h', '24h','2d','3d','1w','2w','1mo']\n",
    "\n",
    "def split_ds( df2 ):\n",
    "    df = df2.filter((pol.col('narrative').str.contains(' LAST ')|\n",
    "                     pol.col('narrative').str.contains('MORNING')|\n",
    "                     pol.col('narrative').str.contains('A.M.',literal=True, strict=True )|\n",
    "                     pol.col('narrative').str.contains('P.M.',literal=True, strict=True )|\n",
    "                     pol.col('narrative').str.contains('AFTERNOON')|\n",
    "                     pol.col('narrative').str.contains(' DAY')|\n",
    "                     pol.col('narrative').str.contains('TODAY') |\n",
    "                     pol.col('narrative').str.contains('EARLIER TONIGHT') |\n",
    "                     pol.col('narrative').str.contains('AROUND') |\n",
    "                     pol.col('narrative').str.contains('YESTERDAY') |\n",
    "                     pol.col('narrative').str.contains(' AGO') )&\n",
    "                   ~(\n",
    "                    pol.col('narrative').str.contains(\"LAST 2 STEPS\", literal=True) |\n",
    "                    pol.col('narrative').str.contains(\"LAST FEW STEP\", literal=True) |\n",
    "                    pol.col('narrative').str.contains(\"LAST STAIR\", literal=True) |\n",
    "                    pol.col('narrative').str.contains(\"LAST SEV STEP\", literal=True) |\n",
    "                    pol.col('narrative').str.contains(\"LAST STEP\", literal=True) |\n",
    "                    pol.col('narrative').str.contains(\"YR AGO\", literal=True) |\n",
    "                    pol.col('narrative').str.contains(\"YRS AGO\", literal=True) |\n",
    "                    pol.col('narrative').str.contains(\"YEAR AGO\", literal=True) |\n",
    "                    pol.col('narrative').str.contains(\"YEARS AGO\", literal=True)\n",
    "                   ))\n",
    "    k  = 'narrative'\n",
    "    kk = 'time2hosp_1'\n",
    "\n",
    "    # Fixed on Nov 20 2023\n",
    "    # df = df2.with_columns( pol.col('treatment_date').str.to_datetime().dt.weekday().alias('weekday') )\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "      pol.col(k).str.contains('SEVERAL HOUR') |\n",
    "      pol.col(k).str.contains('SEV HOUR', literal=True, strict=True) |\n",
    "      pol.col(k).str.contains('SEV. HR', literal=True, strict=True) |\n",
    "      pol.col(k).str.contains('SEV. HOUR', literal=True, strict=True)\n",
    "      ).then(5).otherwise(np.nan).alias( 'a' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "      pol.col(k).str.contains('SEVERAL DAY') | pol.col('narrative_cleaned').str.contains('several day') |\n",
    "      pol.col(k).str.contains('SEV DAY') |\n",
    "      pol.col(k).str.contains('SEVRAL DAY')|\n",
    "      pol.col(k).str.contains('SEV. D')\n",
    "      ).then(5*24).otherwise(np.nan).alias('d' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "      pol.col(k).str.contains('SEVERAL WK') |\n",
    "      pol.col(k).str.contains('SEV. WEEKS') |\n",
    "      pol.col(k).str.contains('SEVERAL WEEK')|\n",
    "      pol.col(k).str.contains('SEV. WK') |\n",
    "      pol.col(k).str.contains('SEV WK')\n",
    "      ).then(5*7*24).otherwise(np.nan).alias('y' ))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('COUPLE HOUR') | pol.col(k).str.contains('COUPLE OF HOUR')\n",
    "      ).then(3).otherwise(np.nan).alias( 'c' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('COUPLE DAY') | pol.col(k).str.contains('COUPLE OF DAY') | pol.col(k).str.contains('3 NITES AGO')\n",
    "      ).then(3*24).otherwise(np.nan).alias( 'f'))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('FEW HOUR')\n",
    "      ).then(4).otherwise(np.nan).alias( 'b' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('FEW DAY') | pol.col(k).str.contains('FEW NIGHT') |  pol.col(k).str.contains('FEW NITE')\n",
    "      ).then(4*24).otherwise(np.nan).alias( 'e' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('FEW HOUR')\n",
    "      ).then(4).otherwise(np.nan).alias( 'ee' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('FEW WEEK')\n",
    "      ).then(4*7*24).otherwise(np.nan).alias( 'eee' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('OTHER DAY')\n",
    "      ).then(9*24).otherwise(np.nan).alias( 'eeee' ))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "      pol.col(k).str.contains('LAST MTH')  | pol.col(k).str.contains('LAST MONTH')\n",
    "      ).then(30*24).otherwise(np.nan).alias( 'g' ))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "      pol.col(k).str.contains('PREVIOUS DAY') | pol.col(k).str.contains('PREV DAY') |\n",
    "      pol.col(k).str.contains('YESTERDAY') | pol.col(k).str.contains('LAST AM')    | pol.col(k).str.contains('LAST PM') |\n",
    "      pol.col(k).str.contains('LAST EVEN') | pol.col(k).str.contains('DAY BEFORE') |\n",
    "      pol.col(k).str.contains('LAST NOC')  | pol.col(k).str.contains('LAST NIGHT') | pol.col(k).str.contains('LAST NITE') | pol.col(k).str.contains('LAST NGHT')\n",
    "      ).then(28).otherwise(np.nan).alias( 'g' ))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "      pol.col(k).str.contains('P.M.', literal=True, strict=True ) |\n",
    "      pol.col(k).str.contains('THIS AFTERNOON') | pol.col(k).str.contains('THIS EVEN') | pol.col(k).str.contains('TONIGHT')\n",
    "      ).then(9).otherwise(np.nan).alias( 'h' ))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('A.M.', literal=True, strict=True )|\n",
    "      pol.col(k).str.contains('MORNING', literal=True, strict=True )\n",
    "      ).then(12).otherwise(np.nan).alias( 'i' ))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('THIS MORNING') |pol.col(k).str.contains('THIS AM') | pol.col(k).str.contains('TDY')|\n",
    "        pol.col(k).str.contains('TODAY')\n",
    "      ).then(12).otherwise(np.nan).alias( 'j' ))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('LAST EVE') | pol.col(k).str.contains('LAST NGIHT') |  pol.col(k).str.contains('AROUND MIDNIGHT') |\n",
    "        pol.col(k).str.contains('LAST WK') | pol.col(k).str.contains('LAST MIGHT') | pol.col(k).str.contains('LAST WEEK') | pol.col(k).str.contains('WK AGO')\n",
    "      ).then(10.5*7).otherwise(np.nan).alias( 'k' ))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('1 day',literal=True, strict=True)\n",
    "      ).then(24).otherwise(np.nan).alias( 'x'))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('1+ day',literal=True, strict=True)\n",
    "      ).then(36).otherwise(np.nan).alias( 'xx'))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('an hour') | pol.col('narrative_cleaned').str.contains('a hour')|\n",
    "        pol.col('narrative_cleaned').str.contains('1 hour')\n",
    "      ).then(1).otherwise(np.nan).alias( 'am'))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('LAST MOND') | pol.col(k).str.contains('MONDAY')\n",
    "      ).then( pol.col('weekday')+7 ).otherwise(np.nan).alias( '1' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('LAST TUE') | pol.col(k).str.contains('TUESDAY')\n",
    "      ).then(pol.col('weekday')+6).otherwise(np.nan).alias( '2' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('LAST WED') | pol.col(k).str.contains('WEDNESDAY')\n",
    "      ).then(pol.col('weekday')+5).otherwise(np.nan).alias( '3' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('LAST THU') | pol.col(k).str.contains('THURSDAY')\n",
    "      ).then(pol.col('weekday')+4).otherwise(np.nan).alias( '4' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('LAST FRI') | pol.col(k).str.contains('FRIDAY')\n",
    "      ).then(pol.col('weekday')+3).otherwise(np.nan).alias( '5' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('LAST SAT') | pol.col(k).str.contains('SATURDAY')\n",
    "      ).then(pol.col('weekday')+2).otherwise(np.nan).alias( '6' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains('LAST SUN') | pol.col(k).str.contains('SUNDAY')\n",
    "      ).then(pol.col('weekday')+1).otherwise(np.nan).alias( '7' ))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('1 mon')  | pol.col('narrative_cleaned').str.contains('a mon')\n",
    "      ).then(24*30).otherwise(np.nan).alias( 'ba'))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('2 month')\n",
    "      ).then(24*60).otherwise(np.nan).alias( 'bb'))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('3 month')\n",
    "      ).then(24*90).otherwise(np.nan).alias( 'bc'))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('4 month')\n",
    "      ).then(24*120).otherwise(np.nan).alias( 'bd'))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('1 week')  | pol.col('narrative_cleaned').str.contains('a week')\n",
    "      ).then(24*7).otherwise(np.nan).alias( 'be'))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('2 week')\n",
    "      ).then(24*14).otherwise(np.nan).alias( 'bf'))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('3 week')\n",
    "      ).then(24*21).otherwise(np.nan).alias( 'bg'))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('4 week')\n",
    "      ).then(24*28).otherwise(np.nan).alias( 'bh'))\n",
    "    df = df.with_columns(pol.when(\n",
    "        pol.col('narrative_cleaned').str.contains('5 week')\n",
    "      ).then(24*35).otherwise(np.nan).alias( 'bi'))\n",
    "\n",
    "    df = df.with_columns(pol.when(\n",
    "      pol.col('narrative').str.contains('SEVERAL MONTH') |\n",
    "      pol.col('narrative').str.contains('SEV MON') |\n",
    "      pol.col('narrative').str.contains('5 MOS AGO')\n",
    "      ).then(24*7*4*5).otherwise(np.nan).alias( 'bj'))\n",
    "\n",
    "    k='narrative'\n",
    "    for n in range(30):\n",
    "        df = df.with_columns(pol.when(\n",
    "          pol.col(k).str.contains(f'{n}D AGO') |\n",
    "          pol.col(k).str.contains(f'{n} DAY') |  pol.col('narrative_cleaned').str.contains(f'{n} day') |\n",
    "          pol.col(k).str.contains(f'{n}DAY')\n",
    "      ).then(n*24).otherwise(np.nan).alias( f'd{n}' ))\n",
    "\n",
    "    k='narrative'\n",
    "    for n in range(24):\n",
    "        df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains(f'AROUND {n}:') |pol.col(k).str.contains(f'AROUND {n}:') |\n",
    "        pol.col(k).str.contains(f'AROUND {n} AM') | pol.col(k).str.contains(f'AROUND {n} PM')\n",
    "      ).then(24).otherwise(np.nan).alias( f'time{n}' ))\n",
    "\n",
    "    k='narrative_cleaned'\n",
    "    for n in range(6): # ================ Month\n",
    "        df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains(f'{n} mth')\n",
    "      ).then(n*30*24).otherwise(np.nan).alias( f'n{n}' ))\n",
    "\n",
    "    for n in range(10): # ================ weeks\n",
    "        df = df.with_columns(pol.when(\n",
    "          pol.col(k).str.contains(f'{n} wk',literal=True, strict=True ) | pol.col(k).str.contains(f'{n}wk',literal=True, strict=True ) |\n",
    "          pol.col(k).str.contains(f'{n} week',literal=True, strict=True ) | pol.col(k).str.contains(f'{n}week',literal=True, strict=True ) |\n",
    "          pol.col('narrative').str.contains(f'{n}WEEK',literal=True, strict=True ) |\n",
    "          pol.col('narrative').str.contains(f'{n}WKS AGO',literal=True, strict=True ) |\n",
    "          pol.col(k).str.contains(f'{n}weeks ago' ) |\n",
    "          pol.col('narrative').str.contains(f'{n} WEEK',literal=True, strict=True )\n",
    "      ).then(n*7*24).otherwise(np.nan).alias( f'n{n}' ))\n",
    "\n",
    "    for n in range(30): # ================ DAY\n",
    "        df = df.with_columns(pol.when(\n",
    "      pol.col(k).str.contains(f'{n}night') |\n",
    "      pol.col(k).str.contains(f'{n} night') |pol.col(k).str.contains(f'{n} d ago') |\n",
    "      pol.col(k).str.contains(f'{n} day') | pol.col(k).str.contains(f'{n} dy ago') |\n",
    "      pol.col(k).str.contains(f'{n}day')\n",
    "      ).then(n*24).otherwise(np.nan).alias( f'n{n}' ))\n",
    "\n",
    "    for n in range(50): # ================ HOUR\n",
    "        df = df.with_columns(pol.when(\n",
    "          pol.col(k).str.contains('hour ago') |\n",
    "          pol.col(k).str.contains(f'{n} hour') |\n",
    "          pol.col(k).str.contains(f'{n}hour') |\n",
    "          pol.col('narrative').str.contains(f'{n} HOURS AGO') |\n",
    "          pol.col(k).str.contains(f'{n}hrs ago') |\n",
    "          pol.col(k).str.contains(f'{n}h ago')\n",
    "      ).then(n).otherwise(np.nan).alias( f'h{n}' ))\n",
    "\n",
    "    for n in range(90): # ================ minutes\n",
    "        df = df.with_columns(pol.when(\n",
    "        pol.col(k).str.contains(f'{n} minute') | pol.col(k).str.contains(f'{n} min ago')\n",
    "      ).then(n/60).otherwise(np.nan).alias( f'm{n}' ))\n",
    "\n",
    "    for n in range(90): # ================ minutes\n",
    "        df = df.with_columns(pol.when(\n",
    "        pol.col('narrative').str.contains(f'{n} MIN')\n",
    "      ).then(n/60).otherwise(np.nan).alias( f'M{n}' ))\n",
    "\n",
    "    rr=-119-90-140\n",
    "    print( 'Sample size:', df.shape, df[:,rr:].head(1) )\n",
    "    time2hosp=np.nanmax( df[:,rr:].to_numpy(),1 )\n",
    "    df = df.with_columns(pol.lit(time2hosp).alias('time2hosp'))\n",
    "    p  = df.filter( pol.col( 'time2hosp') .is_nan() )\n",
    "\n",
    "    print( '\\n\\n',p.shape , 'remaining')\n",
    "    for r in p.sample(5).iter_rows():\n",
    "        print( r[3], )\n",
    "        print( r[31], '\\n' )\n",
    "\n",
    "    p2 = df.filter( pol.col( 'time2hosp') >0 )\n",
    "    trn_df = p2.filter( pol.col('cpsc_case_number').is_in( trn_case_nums ) )\n",
    "    tst_df = p2.filter( pol.col('cpsc_case_number').is_in( tst_case_nums ) )\n",
    "    print( trn_df.shape[0],'dev samples', tst_df.shape[0], 'test samples' )\n",
    "\n",
    "    surv_pols = {}\n",
    "    surv_pols['trn'] = trn_df[0::2,:]\n",
    "    surv_pols['val'] = trn_df[1::2,:]\n",
    "    surv_pols['tst'] = tst_df\n",
    "\n",
    "    return surv_pols\n",
    "\n",
    "if ( 'surv_pols' in globals()) ==False:\n",
    "    surv_pols = split_ds( pol.DataFrame( decoded_df2 ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84439801",
   "metadata": {
    "id": "84439801",
    "papermill": {
     "duration": 0.017804,
     "end_time": "2023-10-06T21:51:26.171648",
     "exception": false,
     "start_time": "2023-10-06T21:51:26.153844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnums={}\n",
    "for t in ['trn','val','tst']:\n",
    "    cnums[t]=np.array(surv_pols[t].select('cpsc_case_number') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd7be5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19dd7be5",
    "outputId": "d93196b0-8e57-4058-c223-b8d4fec7630d",
    "papermill": {
     "duration": 0.017965,
     "end_time": "2023-10-06T21:51:26.197996",
     "exception": false,
     "start_time": "2023-10-06T21:51:26.180031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example\n",
    "np.array([(\"TEXT\", 1, 1), (\"XXX\", 2, 2)], dtype='|S4, i4, i4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e7005",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "da8e7005",
    "outputId": "b8921b9c-0758-41ac-a577-6908b603177b",
    "papermill": {
     "duration": 0.523523,
     "end_time": "2023-10-06T21:51:26.730099",
     "exception": false,
     "start_time": "2023-10-06T21:51:26.206576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoded_df2.drop('Unnamed: 0', axis=1, inplace=True )\n",
    "decoded_df2.drop('Unnamed: 0.1', axis=1, inplace=True )\n",
    "decoded_df2.drop('level_0', axis=1, inplace=True )\n",
    "decoded_df2['cpsc_num'] = decoded_df2.cpsc_case_number.copy()\n",
    "decoded_df2.set_index('cpsc_num',inplace=True)\n",
    "decoded_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dfeb0",
   "metadata": {
    "id": "010dfeb0",
    "papermill": {
     "duration": 0.008783,
     "end_time": "2023-10-06T21:51:26.774187",
     "exception": false,
     "start_time": "2023-10-06T21:51:26.765404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  Calculate severity scores using the AIS model developed by Chung et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036b3f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b036b3f6",
    "outputId": "5fe0eeba-0916-4dad-bc3d-cede4027e2bd",
    "papermill": {
     "duration": 55.166719,
     "end_time": "2023-10-06T21:52:21.949913",
     "exception": false,
     "start_time": "2023-10-06T21:51:26.783194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "### Tensorflow 2.0 version\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras import losses, optimizers, metrics\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "### Define models\n",
    "def define_model(input_size=46, learning_rate=1e-2):\n",
    "    input_data = Input(shape=(input_size, ))\n",
    "\n",
    "    ### Model (Region 46)\n",
    "    x = Dense(64, kernel_regularizer=regularizers.l2(0.0001), kernel_initializer=tf.random_normal_initializer(stddev=0.01))(input_data)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(32, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=x)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=learning_rate), loss=losses.binary_crossentropy)\n",
    "\n",
    "    print('Model==============')\n",
    "    model.summary()\n",
    "\n",
    "    !wget -O Model_Region46.h5 https://raw.githubusercontent.com/HeewonChung92/AIS/main/Model_Region46.h5\n",
    "    model.load_weights( 'Model_Region46.h5')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ais():\n",
    "\n",
    "    def process_dx( df, N=0 ):\n",
    "        bb = pol.DataFrame( df )\n",
    "        bb = bb.with_columns(pol.col(\"narrative\").str.replace_all( r\"(?i)>>|SUSTAINED| D X*|DX:|DX.|DZ:|DZ.|\\b\\/|\\b-\\b\", ' DX. ').alias('processed'))\n",
    "        bb = bb.with_columns(pol.col(\"processed\").str.replace_all( '(>)(>)', ' DX. '))\n",
    "        bb = bb.with_columns(pol.col(\"processed\").str.replace_all( ',,', ' DX. '))\n",
    "        bb = bb.with_columns(pol.col(\"processed\").str.replace_all( '/', ' DX. '))\n",
    "\n",
    "        '''\n",
    "        bb.with_columns(pol.col(\"narrative\").str.replace_all( r\"\\bDX*\", 'DX.'))\n",
    "        bb.with_columns(pol.col(\"narrative\").str.replace_all( \"\\\\***\", 'DX.'))\n",
    "\n",
    "        bb.with_columns(pol.col(\"narrative\").str.replace_all( r\"\\bD X*\", 'DX.'))\n",
    "        bb.with_columns(pol.col(\"narrative\").str.replace_all( \"\\/\", 'DX.'))\n",
    "        bb.with_columns(pol.col(\"narrative\").str.replace_all( ',,', 'DX.'))\n",
    "        ''';\n",
    "        if N>0:\n",
    "            a = bb.filter( ~pol.col('processed').str.contains('DX.') )\n",
    "            print(a.shape[0]/bb.shape[0], 'of samples do not have Dx info;', 1- a.shape[0]/bb.shape[0]  )\n",
    "            for i, r in enumerate( np.array(a.sample(N) ) ):\n",
    "                print(i,r[5], '\\n\\t', r[-1].lower() )\n",
    "        return bb\n",
    "\n",
    "    decoded_pol = process_dx(decoded_df2.copy(), 20 )\n",
    "\n",
    "\n",
    "\n",
    "    k='processed'\n",
    "    def get_dx( df ):\n",
    "        diag_pols = (\n",
    "            df.with_row_count('id').with_columns(pol.col(k).str.split( \"DX.\").alias(\"split_str\"))\n",
    "            .explode(\"split_str\")\n",
    "            .with_columns(\n",
    "                (\"string_\" + pol.arange(0, pol.count()).cast(pol.Utf8).str.zfill(2))\n",
    "                .over(\"id\")\n",
    "                .alias(\"col_nm\")\n",
    "            )\n",
    "            .pivot(\n",
    "                index=['id', k],\n",
    "                values='split_str',\n",
    "                columns='col_nm',\n",
    "            )\n",
    "            .with_columns(\n",
    "                pol.col('^string_.*$').fill_null(\"\")\n",
    "            )\n",
    "        )\n",
    "        return diag_pols\n",
    "\n",
    "    dx_pol = get_dx( decoded_pol )\n",
    "    #dx_pol.head(1)\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    try:\n",
    "        ais_df=pd.read_excel('AIS_codes.xlsx',sheet_name='SimpleCodingBook')\n",
    "    except:\n",
    "        !wget -O AIS_codes.xlsx https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-021-03024-1/MediaObjects/41598_2021_3024_MOESM2_ESM.xlsx\n",
    "        ais_df=pd.read_excel('AIS_codes.xlsx',sheet_name='SimpleCodingBook')\n",
    "\n",
    "    for i, kk in tqdm(enumerate(ais_df['Region-46']) ):\n",
    "        k=kk.upper()\n",
    "        dx_pol = dx_pol.with_columns(pol.when(\n",
    "        pol.col('string_01').str.contains(k) |\n",
    "        pol.col('string_02').str.contains(k) |\n",
    "        pol.col('string_03').str.contains(k) |\n",
    "        pol.col('string_04').str.contains(k) |\n",
    "        pol.col('string_05').str.contains(k) |\n",
    "        pol.col('string_06').str.contains(k) |\n",
    "        pol.col('string_07').str.contains(k) |\n",
    "        pol.col('string_08').str.contains(k) |\n",
    "        pol.col('string_09').str.contains(k) |\n",
    "        pol.col('string_10').str.contains(k) |\n",
    "        pol.col('string_11').str.contains(k) |\n",
    "        pol.col('string_12').str.contains(k)\n",
    "          ).then(1).otherwise(0).alias( f'{k}' ))\n",
    "\n",
    "    ais_features = np.array(dx_pol[:,-46:] )\n",
    "\n",
    "    # add more\n",
    "    for k,c in zip(['HEAD','NECK'], [1,10]):\n",
    "        i=np.array(\n",
    "            decoded_pol.filter( pol.col('processed').str.contains(f'INJURED {k}') | pol.col('processed').str.contains(f'{k} INJU') ).select('index')\n",
    "                  )\n",
    "        print(k, len(i))\n",
    "        ais_features[i, c] = 1 # head\n",
    "\n",
    "    #plt.imshow(  np.array(ais_features[::5000,:]) )\n",
    "    ais_scores = ais_model.predict( ais_features )\n",
    "\n",
    "    return ais_scores, ais_scores.squeeze()\n",
    "\n",
    "ais_model = define_model()\n",
    "\n",
    "ais_scores, ais_scores = get_ais()\n",
    "\n",
    "ais_df = pd.DataFrame( dict( cpsc_nums= decoded_df2.index, ais_scores=ais_scores ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f5151-55a9-4eb9-bcef-25f1e9bbfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram( ais_df.ais_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46939fcb-3a0a-4ca4-8697-9131306aea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram( ais_df[ais_df.ais_scores > 0.04].ais_scores )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df76df3",
   "metadata": {
    "id": "1df76df3",
    "papermill": {
     "duration": 0.024961,
     "end_time": "2023-10-06T21:52:22.145364",
     "exception": false,
     "start_time": "2023-10-06T21:52:22.120403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define t2event given \"timed\" cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd3521",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfcd3521",
    "outputId": "f34a3a28-f224-45f5-f863-75af86ee8541",
    "papermill": {
     "duration": 0.188469,
     "end_time": "2023-10-06T21:52:22.359166",
     "exception": false,
     "start_time": "2023-10-06T21:52:22.170697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_surv( dff, DEFN = 2 ):\n",
    "    from sksurv.util import Surv\n",
    "    ev,time,surv_inter,surv_str={},{},{},{}\n",
    "\n",
    "    if DEFN==1:\n",
    "        thres=3 # treated\n",
    "    elif DEFN==2:\n",
    "        thres=5 # hosp/ died\n",
    "\n",
    "\n",
    "    for t in ['trn','val','tst']:\n",
    "        df=dff[t].to_pandas()\n",
    "        surv_inter[t]=pd.DataFrame( {'label_lower_bound': df['time2hosp'] ,\n",
    "                                     'label_upper_bound': df['time2hosp'] } )\n",
    "\n",
    "        if DEFN==3:\n",
    "            q=np.where(  df['severity'] < thres )[0] # unseen + observed\n",
    "            ev[t] = ( df['severity'] >= thres ).values\n",
    "        else:\n",
    "            q=np.where(  df['severity'] < thres )[0] # unseen + observed\n",
    "            ev[t] = ( df['severity'] >= thres ).values\n",
    "\n",
    "        surv_inter[t].iloc[q, 1] = np.inf\n",
    "        print(t, np.sum(ev[t])/ len(ev[t]), 'n=',len(ev[t]) )\n",
    "        time[t] = ( df['time2hosp']  ).values\n",
    "\n",
    "        surv_str[t]=Surv.from_arrays(ev[t], time[t])\n",
    "    return ev, time, surv_inter, surv_str\n",
    "\n",
    "event_indicator, time2event, surv_inter, surv_str = get_surv( surv_pols, DEFN=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b902e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c84b902e",
    "outputId": "06d0f9ac-b67a-4b9d-a0d6-e953afba369c",
    "papermill": {
     "duration": 0.034567,
     "end_time": "2023-10-06T21:52:22.420729",
     "exception": false,
     "start_time": "2023-10-06T21:52:22.386162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping['body_part']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202ba3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "9202ba3e",
    "outputId": "1c98b960-4d84-4f22-f5ce-bf7bea19999a",
    "papermill": {
     "duration": 2.244278,
     "end_time": "2023-10-06T21:52:24.690180",
     "exception": false,
     "start_time": "2023-10-06T21:52:22.445902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define severity\n",
    "import plotly.express as px\n",
    "t='trn';\n",
    "px.histogram( decoded_df2.loc[cnums[t].squeeze() ], x='body_part' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f70c44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "f0f70c44",
    "outputId": "e7c5cd43-1388-4a07-9a82-2229f5a53242",
    "papermill": {
     "duration": 0.089698,
     "end_time": "2023-10-06T21:52:24.806989",
     "exception": false,
     "start_time": "2023-10-06T21:52:24.717291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define severity !?\n",
    "\n",
    "import plotly.express as px\n",
    "t='trn';\n",
    "px.histogram( decoded_df2.loc[cnums[t].squeeze() ], x='diagnosis' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f75f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "497f75f4",
    "outputId": "d7cf3a8f-9e9e-45b4-9000-c867b025c317",
    "papermill": {
     "duration": 0.074671,
     "end_time": "2023-10-06T21:52:24.908886",
     "exception": false,
     "start_time": "2023-10-06T21:52:24.834215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define severity !?\n",
    "\n",
    "import plotly.express as px\n",
    "t='trn';\n",
    "px.histogram( decoded_df2.loc[cnums[t].squeeze() ], x='disposition' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba2ff9",
   "metadata": {
    "id": "eeba2ff9",
    "papermill": {
     "duration": 0.036221,
     "end_time": "2023-10-06T21:52:24.972403",
     "exception": false,
     "start_time": "2023-10-06T21:52:24.936182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1: unseen\n",
    "2: observed\n",
    "3: treated\n",
    "4: reated\n",
    "5: admitted/hospitalized <-\n",
    "6: died\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eadceb",
   "metadata": {
    "id": "50eadceb",
    "papermill": {
     "duration": 0.027132,
     "end_time": "2023-10-06T21:52:25.081930",
     "exception": false,
     "start_time": "2023-10-06T21:52:25.054798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load pre-computed embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4296f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c4296f7",
    "outputId": "045bfa28-eae2-45f8-b3ab-f78bf2183e70",
    "papermill": {
     "duration": 62.848826,
     "end_time": "2023-10-06T21:53:27.958776",
     "exception": false,
     "start_time": "2023-10-06T21:52:25.109950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "src='narrative_cleaned'\n",
    "EMB=[1,2,3,4]\n",
    "\n",
    "def get_embeddings( EMB, src ):\n",
    "\n",
    "    Embeddings={}\n",
    "    if 'cleaned' in src:\n",
    "        # cpsc_nums = indices not unique and hence 533518\n",
    "        files = ['all_embeddings_1.pkl',\n",
    "                 'all_embeddings_2.pkl',\n",
    "                 'all_embeddings_3.pkl',\n",
    "                 'all_embeddings_4.pkl'\n",
    "                ]\n",
    "        with open(DATA_FOLDER / \"all_cpsc.pkl\", 'rb') as handle:\n",
    "            x=pickle.load(handle)\n",
    "        cpcs_nums= x['cpsc']\n",
    "\n",
    "    else:\n",
    "        files =['', 'narrative_n426691_emb1_d768_2023-10-02.pkl', 'narrative_n426691_emb2_d384_2023-10-02.pkl',\n",
    "            '../input/n-raw-extract-3/narrative_n426691_emb3_d768_2023-10-03.pkl', 'narrative_n426691_emb4_d512_2023-10-02.pkl' ]\n",
    "\n",
    "    for emb in EMB:\n",
    "        def read( file ):\n",
    "            print( file )\n",
    "            with open(DATA_FOLDER / file, 'rb') as handle:\n",
    "                x=pickle.load(handle)\n",
    "            if emb==4:\n",
    "                x=x['embeddings'].numpy()\n",
    "            else:\n",
    "                x=x['embeddings']\n",
    "            return x\n",
    "\n",
    "        x = read(files[emb - 1])\n",
    "        X = pd.DataFrame( np.hstack((np.expand_dims(cpcs_nums,1), x )))\n",
    "        X = X.drop_duplicates(0)\n",
    "        X.set_index(0,inplace=True)\n",
    "\n",
    "        for t in ['trn','val','tst']:\n",
    "            c = surv_pols[t].to_pandas()['cpsc_case_number']\n",
    "            Xs = X.loc[c]\n",
    "            Embeddings[emb,t] = Xs\n",
    "            print(emb, t, Xs.shape)\n",
    "            \n",
    "    return Embeddings,cpcs_nums\n",
    "\n",
    "Embeddings, cpcs_nums = get_embeddings( EMB, src )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993f223",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "3993f223",
    "outputId": "32307234-54bc-420a-a222-d93199721d59",
    "papermill": {
     "duration": 0.055963,
     "end_time": "2023-10-06T21:53:28.043712",
     "exception": false,
     "start_time": "2023-10-06T21:53:27.987749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t in ['trn','val','tst']:\n",
    "    s= np.array( surv_pols[t].select( 'year' )) - 2013\n",
    "    #surv_pols[t].with_columns( (pol.col('year')-pol.lit(2013) ).alias('year') )\n",
    "    surv_pols[t].replace( 'year', pol.Series(s[:,0])  )\n",
    "surv_pols[t].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e5454",
   "metadata": {
    "id": "fc1e5454",
    "papermill": {
     "duration": 0.028099,
     "end_time": "2023-10-06T21:53:28.165006",
     "exception": false,
     "start_time": "2023-10-06T21:53:28.136907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the pre-computed OpenAI embeddings provided by DrivenData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc7fd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bcc7fd9",
    "outputId": "1e044407-6ec2-42f1-bd10-32e10bbfc468",
    "papermill": {
     "duration": 12.000927,
     "end_time": "2023-10-06T21:54:08.508015",
     "exception": false,
     "start_time": "2023-10-06T21:53:56.507088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_embeddings = pd.read_parquet(DATA_FOLDER / \"openai_embeddings_primary_narratives.parquet.gzip\")\n",
    "openai_embeddings.set_index('cpsc_case_number',inplace=True)\n",
    "\n",
    "emb=11\n",
    "for t in ['trn', 'val', ]:\n",
    "    s=openai_embeddings.loc[cnums[t].squeeze()]['embedding']\n",
    "    Embeddings[emb,t]=np.reshape( np.concatenate(s.values).ravel(), (len(s),len(s.values[1]) ) )\n",
    "    assert (s.values[0] - np.reshape( np.concatenate(s.values).ravel(), (len(s),len(s.values[1]) ) )[0,:] ).sum() ==0\n",
    "    print('loaded OpenAI embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a7735-bc9b-4048-b91c-44abf30d3836",
   "metadata": {},
   "source": [
    "## Read in LEALLA embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88382ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e88382ae",
    "outputId": "5fd1d1e2-3f46-4463-b1cc-4613f0264c77",
    "papermill": {
     "duration": 3.619693,
     "end_time": "2023-10-06T21:54:12.226350",
     "exception": false,
     "start_time": "2023-10-06T21:54:08.606657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb = 6\n",
    "def read_6():\n",
    "    # Read in LEALLA embeddings\n",
    "    try:\n",
    "        cpcs_nums= decoded_df2.cpsc_case_number\n",
    "    except:\n",
    "        cpcs_nums= decoded_df2.index\n",
    "    emb = 6\n",
    "    for d in range(20):\n",
    "        files = [\n",
    "        'narrative_cleaned_n426691_emb6_d0_2023-10-04.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d1_2023-10-04.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d2_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d3_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d4_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d5_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d6_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d7_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d8_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d9_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d10_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d11_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d12_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d13_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d14_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d15_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d16_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d17_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d18_2023-10-05.pkl',\n",
    "        'narrative_cleaned_n426691_emb6_d19_2023-10-05.pkl' ]\n",
    "\n",
    "        with open(DATA_FOLDER / \"LEALLA\" / files[d], 'rb') as handle:\n",
    "            x=pickle.load(handle)\n",
    "\n",
    "        print('read embeddings', files[d] )\n",
    "\n",
    "        x=x['embeddings']\n",
    "\n",
    "        nfeats=x.shape[1]\n",
    "        try:\n",
    "            all_embeddings;\n",
    "        except:\n",
    "            all_embeddings = np.zeros( (decoded_df2.shape[0],nfeats) )\n",
    "\n",
    "        all_embeddings[d::20,:] = x\n",
    "\n",
    "    print( all_embeddings.shape )\n",
    "    X = pd.DataFrame( np.hstack((np.expand_dims(cpcs_nums,1), all_embeddings )))\n",
    "    X.set_index(0,inplace=True)\n",
    "    for t in ['trn','val','tst']:\n",
    "        c = surv_pols[t].to_pandas()['cpsc_case_number']\n",
    "        Xs = X.loc[c]\n",
    "        Embeddings[emb,t] = Xs\n",
    "        print(emb, t, Xs.shape)\n",
    "\n",
    "read_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c5872",
   "metadata": {
    "id": "fc1c5872",
    "papermill": {
     "duration": 0.033712,
     "end_time": "2023-10-06T21:54:12.370301",
     "exception": false,
     "start_time": "2023-10-06T21:54:12.336589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fit dimensionality-reduction models using a subset of the computed word embeddings (via UMAP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5232705",
   "metadata": {
    "id": "d5232705",
    "papermill": {
     "duration": 0.040685,
     "end_time": "2023-10-06T21:54:12.301231",
     "exception": false,
     "start_time": "2023-10-06T21:54:12.260546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMB = [1,2,3,4,6,11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j1_ZZ_1DUgOR",
   "metadata": {
    "id": "j1_ZZ_1DUgOR"
   },
   "source": [
    "## Visualize the reduced word embeddings, stratified by select categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c018afd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c018afd",
    "outputId": "f5307223-0cff-4121-ad51-3c3fe7aa52d5",
    "papermill": {
     "duration": 244.063947,
     "end_time": "2023-10-06T21:58:16.467672",
     "exception": false,
     "start_time": "2023-10-06T21:54:12.403725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import umap\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t ='trn'\n",
    "\n",
    "reducers = {}\n",
    "for rdims in [4]: # use more than 2 dims per https://programminghistorian.org/en/lessons/clustering-visualizing-word-embeddings\n",
    "    for e in EMB:\n",
    "        reducers[e,rdims] = umap.UMAP(\n",
    "        n_neighbors=25,\n",
    "        min_dist=0.01,\n",
    "        n_jobs=1,\n",
    "        n_components=rdims,\n",
    "        random_state=1119).fit( Embeddings[e,t] )\n",
    "        print('Reducing word embedding', e, 'via UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pqFzvdk24abb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqFzvdk24abb",
    "outputId": "1b2492c0-622f-4419-aa6e-b184636899db"
   },
   "outputs": [],
   "source": [
    "reducers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MsvG_KM_6nUZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MsvG_KM_6nUZ",
    "outputId": "91ab157a-ca6d-479d-9599-27435723c48c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# same dimensions\n",
    "def show_umaps( mid, rdims ):\n",
    "    t='trn'\n",
    "    X_embedded = reducers[mid,rdims].transform( Embeddings[mid,t]  )\n",
    "    embedded_dict = {}\n",
    "    for i in range(0,X_embedded.shape[1]):\n",
    "        embedded_dict[f\"Dim {i+1}\"] = X_embedded[:,i]\n",
    "\n",
    "    projected = pd.DataFrame( embedded_dict )\n",
    "    projected['cids']= cnums[t]\n",
    "\n",
    "    for k in ['sex', 'age_cate_binned','body_part','location', 'diagnosis','race_white','race_4', 'severity']:\n",
    "        projected[k]= surv_pols[t].to_pandas()[k]\n",
    "\n",
    "    fname = 'DejaVu Sans'\n",
    "    tfont = {'fontname':fname, 'fontsize':12} # title font attributes\n",
    "    afont = {'fontname':fname, 'fontsize':10} # axis font attributes\n",
    "    lfont = {'fontname':fname, 'fontsize':8}  # legend font attributes\n",
    "\n",
    "    def tune_figure(ax, title:str='Title'):\n",
    "        ax.axis('off')\n",
    "\n",
    "        title =f'WB{mid} colored by {title}'\n",
    "        ax.set_title(title, **tfont)\n",
    "        ax.get_legend().set_title(\"\")\n",
    "        ax.get_legend().prop.set_family(lfont['fontname'])\n",
    "        ax.get_legend().prop.set_size(lfont['fontsize'])\n",
    "        ax.get_legend().get_frame().set_linewidth(0.0)\n",
    "\n",
    "    f, axs = plt.subplots(3,2,figsize=(14,7))\n",
    "    try:\n",
    "        axs = axs.flatten()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    d,k=3,'location'\n",
    "    sns.scatterplot(data=projected, x='Dim 1', y='Dim 2', hue=k, s=50, alpha=0.1, ax=axs[d]);\n",
    "    tune_figure(axs[d], k)\n",
    "\n",
    "    d,k=0,'severity'\n",
    "    sns.scatterplot(data=projected, x='Dim 2', y='Dim 3', hue=k, s=50, alpha=0.1, ax=axs[d]);\n",
    "    tune_figure(axs[d], k)\n",
    "\n",
    "    d,k=1,'sex'\n",
    "    sns.scatterplot(data=projected, x='Dim 3', y='Dim 1', hue=k, s=50, alpha=0.1, ax=axs[d]);\n",
    "    tune_figure(axs[d], k)\n",
    "    d,k=2,'body_part'\n",
    "    sns.scatterplot(data=projected, x='Dim 1', y='Dim 2', hue=k, s=50, alpha=0.1, ax=axs[d]);\n",
    "    tune_figure(axs[d], k)\n",
    "\n",
    "    d,k=4,'race_4'\n",
    "    sns.scatterplot(data=projected, x='Dim 2', y='Dim 3', hue=k, s=50, alpha=0.1, ax=axs[d]);\n",
    "    tune_figure(axs[d], k)\n",
    "\n",
    "    d,k=5,'race_white'\n",
    "    sns.scatterplot(data=projected, x='Dim 3', y='Dim 1', hue=k, s=50, alpha=0.1, ax=axs[d]);\n",
    "    tune_figure(axs[d], k)\n",
    "\n",
    "# viewing different combination (including 3rd and 4th dims)\n",
    "[show_umaps( s, r) for r in [4] for s in EMB]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295eeed8",
   "metadata": {
    "id": "295eeed8",
    "papermill": {
     "duration": 0.269572,
     "end_time": "2023-10-06T21:59:40.266308",
     "exception": false,
     "start_time": "2023-10-06T21:59:39.996736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transform the rest of the (test) word embeddings using the fitted UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721d6ad",
   "metadata": {
    "id": "d721d6ad",
    "papermill": {
     "duration": 409.896512,
     "end_time": "2023-10-06T22:06:30.409695",
     "exception": false,
     "start_time": "2023-10-06T21:59:40.513183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_reduced={}\n",
    "for emb in EMB:\n",
    "    T = ['trn', 'val','tst']\n",
    "    if emb == 11:\n",
    "        T = ['trn','val']\n",
    "    if emb>0:\n",
    "        for r in [4]:\n",
    "            for t in T:\n",
    "                word_reduced[emb,t] = reducers[emb,r].transform( Embeddings[emb,t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N4xwozzmUsch",
   "metadata": {
    "id": "N4xwozzmUsch"
   },
   "source": [
    "# More preliminary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5626e4",
   "metadata": {
    "id": "ac5626e4",
    "papermill": {
     "duration": 0.217556,
     "end_time": "2023-10-06T22:06:31.721802",
     "exception": false,
     "start_time": "2023-10-06T22:06:31.504246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Examine lengths of narratives (400-word narrative only introduced in a later time period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8c62e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8ab8c62e",
    "outputId": "3cac00df-d695-41d9-8fcc-4a01402e3b9f",
    "papermill": {
     "duration": 0.420113,
     "end_time": "2023-10-06T22:06:32.370707",
     "exception": false,
     "start_time": "2023-10-06T22:06:31.950594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t in ['trn','tst']:\n",
    "    surv_pols[t]= surv_pols[t].with_columns([\n",
    "        pol.col(\"narrative\").str.len_bytes().alias(\"narrative_len\")\n",
    "    ])\n",
    "    fig=px.histogram( surv_pols[t].to_pandas(),'narrative_len')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112aaf78",
   "metadata": {
    "id": "112aaf78",
    "papermill": {
     "duration": 0.226061,
     "end_time": "2023-10-06T22:06:32.835280",
     "exception": false,
     "start_time": "2023-10-06T22:06:32.609219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Develop and evaluate survival analysis models based on eXtreme Gradient Boost with hyperparameter optimization using the ```optuna``` package\n",
    "\n",
    "\n",
    "- The loop below will walk through a series of trials to collect statistics that quantify illustrate the effects of input data types to each survival model candidate\n",
    "- Within each trial, we use the ```optuna``` object, via ```run_xgb_optuna```  function wherein the optuna object will conduct Bayesian search with steps equal to the value of ```n_trials```\n",
    "\n",
    "- **Warning**: this section will take some time. For a quick dry-run, change these values:\n",
    "  ```n_trials= (100 if INTERACTIVE else 500)```\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461221b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0461221b",
    "outputId": "6371ccdd-0cd1-4b50-9154-3927dee1076a",
    "papermill": {
     "duration": 0.245387,
     "end_time": "2023-10-06T22:06:33.309925",
     "exception": false,
     "start_time": "2023-10-06T22:06:33.064538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_ratio = 1-np.isinf( surv_inter['trn']['label_upper_bound'] ).sum() / surv_inter['trn'].shape[0]\n",
    "print( 'pos-neg-ratio:', pos_ratio,  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7A_8vDdE0yek",
   "metadata": {
    "id": "7A_8vDdE0yek"
   },
   "source": [
    "## Adjust ```n_trials``` hyperparamter for fine-grained (or coarse) hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a421e8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a421e8e",
    "outputId": "bad57003-0e29-4295-d165-37c4a5f0eabc",
    "papermill": {
     "duration": 0.24718,
     "end_time": "2023-10-06T22:06:33.774061",
     "exception": false,
     "start_time": "2023-10-06T22:06:33.526881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_trials = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NHvoqOrS0yhS",
   "metadata": {
    "id": "NHvoqOrS0yhS"
   },
   "source": [
    "## Define function that calls the ```optuna``` object performing hyperparameter optimization of model parameters involved in each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7LQ7nr4I0u1u",
   "metadata": {
    "id": "7LQ7nr4I0u1u"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "def run_xgb_optuna( T, emb, X, surv_inter ):\n",
    "    ds = {}\n",
    "    base_params = {'verbosity': 0,\n",
    "                  'objective': 'survival:aft',\n",
    "                  'eval_metric': 'aft-nloglik',\n",
    "                  'tree_method': 'hist'}  # Hyperparameters common to all trials\n",
    "    samp_choices = ['uniform']\n",
    "\n",
    "    for t in T:\n",
    "        ds[t] = xgb.DMatrix( X[t])\n",
    "        # see details https://xgboost.readthedocs.io/en/stable/tutorials/aft_survival_analysis.html\n",
    "        ds[t].set_float_info('label_lower_bound', surv_inter[t]['label_lower_bound'] )\n",
    "        ds[t].set_float_info('label_upper_bound', surv_inter[t]['label_upper_bound'] )\n",
    "\n",
    "    t='trn1'\n",
    "    print(X['trn'][0::2,:].shape )\n",
    "    ds[t] = xgb.DMatrix( X['trn'][0::2,:] )\n",
    "    ds[t].set_float_info('label_lower_bound', surv_inter['trn']['label_lower_bound'][0::2])\n",
    "    ds[t].set_float_info('label_upper_bound', surv_inter['trn']['label_upper_bound'][1::2])\n",
    "    t='trn2'\n",
    "    ds[t] = xgb.DMatrix( X['trn'][1::2,:] )\n",
    "    ds[t].set_float_info('label_lower_bound', surv_inter['trn']['label_lower_bound'][0::2])\n",
    "    ds[t].set_float_info('label_upper_bound', surv_inter['trn']['label_upper_bound'][1::2])\n",
    "\n",
    "    if gpus:\n",
    "        base_params.update( {'tree_method': 'gpu_hist', 'device':'cuda', } )\n",
    "        samp_choices = ['gradient_based','uniform']\n",
    "\n",
    "    def tuner(trial):\n",
    "        params = {'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0),\n",
    "                  'aft_loss_distribution': trial.suggest_categorical('aft_loss_distribution',\n",
    "                                                                      ['normal', 'logistic', 'extreme']),\n",
    "                  'aft_loss_distribution_scale': trial.suggest_float('aft_loss_distribution_scale', 0.1, 10.0),\n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                  'booster': trial.suggest_categorical('booster',['gbtree','dart',]),\n",
    "                  'scale_pos_weight': trial.suggest_float('scale_pos_weight', pos_ratio*0.1, 10*pos_ratio ),  # L1 reg on weights\n",
    "                  'alpha': trial.suggest_float('alpha', 1e-8, 10 ),  # L1 reg on weights\n",
    "                  'lambda': trial.suggest_float('lambda', 1e-8, 10 ),  # L2 reg on weights\n",
    "                  'eta': trial.suggest_float('eta', 0, 1.0),  # step size\n",
    "                  'sampling_method': trial.suggest_categorical('sampling_method', samp_choices ),\n",
    "                  'subsample': trial.suggest_float('subsample', 0.01, 1 ),\n",
    "                  'gamma': trial.suggest_float('gamma', 1e-8, 10)  # larger, more conservative; min loss reduction required to make leaf\n",
    "        }\n",
    "\n",
    "        params.update(base_params)\n",
    "        pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'valid-aft-nloglik')\n",
    "\n",
    "        bst = xgb.train(params, ds['trn1'], num_boost_round=10000,\n",
    "                        evals=[(ds['trn1'], 'train'), (ds['trn2'], 'valid')],  # <---- data matrices\n",
    "                        early_stopping_rounds=50, verbose_eval=False, callbacks=[pruning_callback])\n",
    "        if bst.best_iteration >= 25:\n",
    "            return bst.best_score\n",
    "        else:\n",
    "            return np.inf  # Reject models with < 25 trees\n",
    "\n",
    "    # Run hyperparameter search\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize( tuner, n_trials= n_trials )\n",
    "    print('Completed hyperparameter tuning with best aft-nloglik = {}.'.format(study.best_trial.value))\n",
    "    params = {}\n",
    "    params.update(base_params)\n",
    "    params.update(study.best_trial.params)\n",
    "\n",
    "    print('Re-running the best trial... params = {}'.format(params))\n",
    "    bst = xgb.train(params, ds['trn1'], num_boost_round=10000, verbose_eval=False,\n",
    "                    evals=[(ds['trn1'], 'train'), (ds['trn2'], 'valid')],\n",
    "                    early_stopping_rounds=50)\n",
    "\n",
    "    # Explore hyperparameter search space\n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    fig.show()\n",
    "\n",
    "    for t in T:\n",
    "        try:\n",
    "            res[t]= pd.DataFrame({'Label (lower bound)': surv_inter[t]['label_lower_bound'],\n",
    "                       'Label (upper bound)': surv_inter[t]['label_upper_bound'],\n",
    "                       'Predicted label': bst.predict(ds[t]) } )\n",
    "\n",
    "            sp=scipy.stats.spearmanr( res[t].iloc[:,-2], res[t].iloc[:,-1] )\n",
    "            c=concordance_index_censored( event_time = time2event[t], event_indicator = event_indicator[t] , estimate=1/res[t].iloc[:,-1] )\n",
    "\n",
    "            print(t.upper(), f'| R2:{sp[0]:.3f}; p:{sp[1]:.4} | C:{c[0]*100:.2f} ', end='|' )\n",
    "            for d,h in enumerate([3,6,9,12,15,18,24,49,73, 7*24+1, 7*24*2+1, 7*24*4+1 ]):\n",
    "                bs = brier_score( surv_str['trn'], surv_str[t], estimate=1/res[t].iloc[:,-1], times=[h] )\n",
    "                print( end=f'{labels[d]}:{bs[1][0]:.3f} |' )\n",
    "            print()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    today = date.today()\n",
    "    bst.save_model(f'aft_model_{emb}_{today}.json')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uWYnjLjPzjQd",
   "metadata": {
    "id": "uWYnjLjPzjQd"
   },
   "source": [
    "## Loop over a series of trials to compare the effects of input data types\n",
    "\n",
    "The for-loop below explores input types, depending on ```input_type```:\n",
    "- ```input_type=1 # 2, 3, 4```: original word embedding\n",
    "- ```input_type=21 # 22, 23, 24, 26 ```: dimensionality reduced versions of word embedding\n",
    "- ```input_type=25```: all dimensionality reduced versions of word embedding, *plus* baseline variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d2d56-eb75-4128-ad49-ac30f352df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = False\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823555c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5823555c",
    "outputId": "5e5a2d61-d118-45a0-c322-a322c36d5440",
    "papermill": {
     "duration": 96.322997,
     "end_time": "2023-10-06T22:08:10.312036",
     "exception": false,
     "start_time": "2023-10-06T22:06:33.989039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Begin XGB/ Optuna search\n",
    "# ====================================\n",
    "pos_ratio = 1-np.isinf( surv_inter['trn']['label_upper_bound'] ).sum() / surv_inter['trn'].shape[0]\n",
    "print( 'pos-neg-ratio:', pos_ratio,  )\n",
    "\n",
    "for input_type in [25]:  # part 1 : 6,19,20,\n",
    "    X,res ={},{}\n",
    "    if input_type==11:\n",
    "        T=['trn','val']\n",
    "    else:\n",
    "        T=['trn','val','tst']\n",
    "    for t in T:\n",
    "        if input_type == 19:\n",
    "            X[t] = np.hstack( (Embeddings[1,t],Embeddings[2,t],Embeddings[3,t], Embeddings[4,t], Embeddings[6,t] ) )\n",
    "        elif input_type == 20:\n",
    "            X[t] = np.hstack( (X[t], surv_pols[t][att].to_pandas()) )\n",
    "        elif input_type == 25:\n",
    "            X[t] = np.hstack( (word_reduced[1,t],word_reduced[2,t],word_reduced[3,t],word_reduced[4,t],word_reduced[6,t], surv_pols[t][att].to_pandas()  ) )\n",
    "        elif input_type >= 21: # dim reduced versions for comparison with Cox regression to be done next code block\n",
    "            rr = input_type - 20\n",
    "            X[t] = np.hstack( (word_reduced[rr,t], surv_pols[t][att].to_pandas() ) )\n",
    "        else: # 1-4,6,11\n",
    "            try:\n",
    "                X[t] = Embeddings[input_type,t].to_numpy()\n",
    "            except:\n",
    "                print(emb,'skip numpy')\n",
    "                X[t] = Embeddings[input_type,t]\n",
    "    res = run_xgb_optuna( T, input_type, X, surv_inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55126e",
   "metadata": {
    "id": "5b55126e",
    "papermill": {
     "duration": 0.250955,
     "end_time": "2023-10-06T22:08:11.317184",
     "exception": false,
     "start_time": "2023-10-06T22:08:11.066229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cox regression analysis for baseline comparison and preliminary interpretations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d98cd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "62d98cd1",
    "outputId": "a4398286-633f-4167-d968-758b709199e7",
    "papermill": {
     "duration": 4.363515,
     "end_time": "2023-10-06T22:08:15.938465",
     "exception": false,
     "start_time": "2023-10-06T22:08:11.574950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from lifelines import AalenAdditiveFitter, CoxPHFitter\n",
    "\n",
    "att =['location','product_1','product_2','product_3','fire_involvement','body_part','drug','alcohol', 'sex', 'age_cate_binned','race_recoded','year','month']\n",
    "\n",
    "att1=att.copy() # without race\n",
    "att1+=['time2hosp', 'severity']\n",
    "att1.remove('fire_involvement')\n",
    "att1.remove('race_recoded')\n",
    "\n",
    "att2=att.copy()  # with race\n",
    "att2+=['time2hosp', 'severity']\n",
    "att2.remove('fire_involvement')\n",
    "print( 'Attribute subsets\\natt1:',att1, '\\natt2:', att2 )\n",
    "\n",
    "import sksurv\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.util import Surv\n",
    "from lifelines import CoxPHFitter\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "from lifelines.utils import k_fold_cross_validation\n",
    "\n",
    "def run_cox():\n",
    "    cphs={}\n",
    "    for emb in [-1,0,1,2,3,4,6,11]:\n",
    "        T = ['trn', 'val','tst']\n",
    "        if emb == 11:\n",
    "            T = ['trn','val']\n",
    "\n",
    "        TIMES = [3,6,9,12,15,18,24,49,73, 7*24+1, 7*24*2+1, 7*24*4+1 ]\n",
    "        trn_event_times = np.array(surv_pols['trn']['time2hosp'].unique())\n",
    "\n",
    "        M = len(trn_event_times)\n",
    "\n",
    "        a=np.broadcast_to( trn_event_times[:, np.newaxis], (M, len(TIMES)) )\n",
    "        b=np.broadcast_to( TIMES, (M, len(TIMES) ) )\n",
    "        Q=np.unique( np.argmin( (a - b)**2 ,axis=1))\n",
    "\n",
    "        print('-'*100, emb, '\\n\\n')\n",
    "        for t in T:\n",
    "            if emb==-1:\n",
    "                A = att2\n",
    "            else:\n",
    "                A = att1\n",
    "            XX =surv_pols[t].to_pandas()[A] *1.\n",
    "\n",
    "            if emb>0:\n",
    "                XX = pd.DataFrame( np.hstack( (XX, word_reduced[emb,t]) ), columns=A +['w1','w2','w3', 'w4'] )\n",
    "\n",
    "            if t == 'trn':\n",
    "                penalty = np.ones( XX.shape[1]-2 )*.1\n",
    "\n",
    "                aaf_1 = AalenAdditiveFitter(coef_penalizer=0.5)\n",
    "                aaf_2 = AalenAdditiveFitter(coef_penalizer=10)\n",
    "                cph = CoxPHFitter(penalizer=penalty, l1_ratio=.1)\n",
    "\n",
    "                if 0:\n",
    "                    print(np.mean(k_fold_cross_validation(cph, XX, duration_col='time2hosp', event_col='severity',)))# scoring_method=\"concordance_index\")))\n",
    "                    print(np.mean(k_fold_cross_validation(aaf_1, XX, duration_col='time2hosp', event_col='severity',)))# scoring_method=\"concordance_index\")))\n",
    "                    print(np.mean(k_fold_cross_validation(aaf_2, XX, duration_col='time2hosp', event_col='severity',)))# scoring_method=\"concordance_index\")))\n",
    "\n",
    "                cphs[emb] = CoxPHFitter(penalizer=penalty, l1_ratio=.1)\n",
    "                cphs[emb].fit(XX, 'time2hosp', 'severity')\n",
    "\n",
    "            preds = cphs[emb].predict_cumulative_hazard( XX ).iloc[Q,:]\n",
    "            c=concordance_index_censored(event_time = time2event[t],\n",
    "                                         event_indicator = event_indicator[t],\n",
    "                                         estimate=preds.iloc[-1,:] )\n",
    "\n",
    "            sp=scipy.stats.spearmanr( np.array( XX['time2hosp']), 1/ preds.iloc[-1,:].values  )\n",
    "\n",
    "            print(f'InputSet{emb}\\n', t.upper(), f'| R2:{sp[0]:.3f}; p:{sp[1]:.4} | C:{c[0]*100:.2f} ', )\n",
    "\n",
    "            for d,(ii,h) in enumerate(zip(Q,TIMES)):\n",
    "                try:\n",
    "                    bs = brier_score( surv_str['trn'], surv_str[t], estimate= preds.iloc[ii,:], times=[h] )\n",
    "                    print( end=f'{labels[d]}:{bs[1][0]:.3f} | ' )\n",
    "                except:\n",
    "                    pass\n",
    "            c=cphs[emb].score( XX,'concordance_index')\n",
    "            print( f'\\t{c:.3f}' )\n",
    "        print('\\n\\n')\n",
    "        cphs[emb].print_summary()\n",
    "        cphs[emb].plot()\n",
    "        plt.figure()\n",
    "        return cphs\n",
    "\n",
    "cphs = run_cox()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JavDIt9fd2GQ",
   "metadata": {
    "id": "JavDIt9fd2GQ"
   },
   "source": [
    "## Interpreting the result outputs\n",
    "\n",
    "\n",
    "Example output:\n",
    "```\n",
    "InputSet-1\n",
    " TRN | R2:0.184; p:1.201e-39 | C:58.23\n",
    "3h:0.997 | 6h:0.996 | 9h:0.990 | 12h:0.858 | 15h:0.855 | 18h:0.853 | 24h:0.844 | 2d:0.634 | 3d:0.580 | 1w:0.387 | \t0.576\n",
    "InputSet-1\n",
    " VAL | R2:0.156; p:5.188e-29 | C:57.99\n",
    "3h:0.998 | 6h:0.995 | 9h:0.989 | 12h:0.852 | 15h:0.849 | 18h:0.847 | 24h:0.833 | 2d:0.616 | 3d:0.563 | 1w:0.364 | \t0.564\n",
    "InputSet-1\n",
    " TST | R2:0.148; p:1.226e-104 | C:55.31\n",
    "3h:1.000 | 6h:0.998 | 9h:0.995 | 12h:0.931 | 15h:0.928 | 18h:0.926 | 24h:0.917 | 2d:0.758 | 3d:0.711 | 1w:0.504 | \t0.559\n",
    "```\n",
    "\n",
    "- Overall concordant indices for the development set (```TRN``` and ```VAL```) and evaluation set (```TST```) are 0.582, 0.580, 0.553, respectively.\n",
    "\n",
    "- Concordant indices for predicting risk of experience the outcome within 12 hours' time: 0.858, 0. 0.852, 0.931  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c11992-2f1f-4f31-abc1-544375c00fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1108.159934,
   "end_time": "2023-10-06T22:08:24.985574",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-06T21:49:56.825640",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
