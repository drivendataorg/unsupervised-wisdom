{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ebfc6-8e46-4c36-a110-0ad9ce1a54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OPENAI_API_KEY\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"set your api key here.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89811686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "\n",
    "# config\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3dbb9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fddc67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"./dataset/primary_data.csv\",)\n",
    "\n",
    "# preprocess\n",
    "df[\"narrative\"] = df[\"narrative\"].map(lambda x:x.lower())\n",
    "\n",
    "# random sampling\n",
    "df = df.sample(n=300, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f74db4-b5ab-45c2-bccb-b5a0362302a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./dataset/variable_mapping.json','r',encoding='utf-8') as file:\n",
    "    mapping = json.load(file)\n",
    "    \n",
    "# convert the encoded values in the mapping to integers since they get read in as strings\n",
    "for c in mapping.keys():\n",
    "    mapping[c] = {int(k): v for k, v in mapping[c].items()}\n",
    "    \n",
    "for col in mapping.keys():\n",
    "    df[col] = df[col].map(mapping[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e9a37-6ede-4898-85af-b58ade5a28fa",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "\n",
    "While the \"Product\" column lists the items that caused falls, it doesn't explain how these items led to the falls. This gap necessitates an exploration of the underlying factors. We aim to speculate on these causative factors.\n",
    "\n",
    "### Integration of Narrative and Tabular Information (age, sex, race, product)\n",
    "\n",
    "The \"Product\" column mentions the objects involved in the incidents, but lacks details on how these objects caused falls. Without this explanation, it's challenging to devise effective strategies. Our focus under \"Narrative Info\" is on the question: `\"Why did the fall occur?\"`. These incidents often happen during `some activity`. Identifying activities unsuitable for the elderly could enable others to provide better support or inform product design. \n",
    "\n",
    "**Although this is a simplistic approach, gaining insights into precursor events or other relevant information from narratives is an essential step.**\n",
    "\n",
    "Simple formula:\n",
    "\n",
    "$$\n",
    "\\text{Narrative} - \\text{product} - \\text{injury} \\cdots = \\text{Insights that only narratives have?}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffbe29a-8c67-4e3d-8010-e7119c6e4be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Morphological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f35fdd-b5ff-489f-a350-f0074d5b9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def extract_verbs(sentence):\n",
    "    \n",
    "    sentence = sentence.lower() # 小文字化\n",
    "    tokens = word_tokenize(sentence) # 文をトークン化\n",
    "    tagged_tokens = pos_tag(tokens) # トークンに品詞タグを付ける\n",
    "    \n",
    "    # 動詞のみを取り出す (動名詞や過去分詞形を除外)\n",
    "    # verbs = [word for word, tag in tagged_tokens if tag.startswith('VB') and tag not in ['VBG', 'VBN']]\n",
    "    verbs = [word for word, tag in tagged_tokens if tag.startswith('VB') or tag.startswith('VBG') or tag.startswith('VBN')]\n",
    "\n",
    "    return verbs\n",
    "\n",
    "verbs_morph = df[\"narrative\"].map(extract_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ff9cf-fe05-4db2-b3ca-6d31c42817e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization_word_list(word_list):\n",
    "    ret=[]\n",
    "    for word in word_list:\n",
    "        ret.append(lemmatizer.lemmatize(word,pos=\"v\"))\n",
    "    return ret\n",
    "\n",
    "verbs_morph2 = [lemmatization_word_list(ws) for ws in verbs_morph]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140267b2-a1f4-4b4c-ab66-36bd1c9fa8f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce84e8-3376-4ac5-aa25-8ae3a22c97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc955c8-39dd-40df-93c9-9c44209f5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"text\": '94YOM FELL TO THE FLOOR AT THE NURSING HOME ONTO BACK OF HEAD SUSTAINED A SUBDURAL HEMATOMA',\n",
    "        \"verb\": \"FELL TO THE FLOOR\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": '87YOF WAS GETTING UP FROM THE COUCH AND FELL TO THE FLOOR SUSTAINED ABRASIONS TO ELBOWS ADMITTED FOR HEMORRHAGIC STROKE',\n",
    "        \"verb\": \"GETTING UP FROM THE COUCH\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": '67YOF WAS AT A FRIENDS HOUSE AND  THAT WAS ON THE FLOOR AND SUSTAINED A RIGHT RADIUS FX',\n",
    "        \"verb\": \"SLIPPED ON WATER\"\n",
    "    },\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "text: {text}\n",
    "verb: {verb}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    template=example_formatter_template,\n",
    "    input_variables=[\"text\", \"verb\"]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"These are sentences describing situations when elderly individuals fell. Please extract from the sentences the causes of the falls and the actions the elderly individuals took when they fell.\",\n",
    "    suffix=\"text: {input}\\nverb:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    "\n",
    ")\n",
    "    \n",
    "# ChatGPT Model\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\",temperature=0)\n",
    "\n",
    "def extract_verbs_with_chatgpt(sentence):\n",
    "    \n",
    "    prompt_text = few_shot_prompt.format(input=sentence)\n",
    "    answer = llm(prompt_text)\n",
    "    print(f\"TEXT: {sentence}\")\n",
    "    print(f\"VERB: {answer}\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ba182-8f19-414b-b970-474520ff58e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbs_chatgpt = df[\"narrative\"].map(extract_verbs_with_chatgpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce8383-d7d6-49e7-94f0-61531d7bae3f",
   "metadata": {},
   "source": [
    "<font size=4 color=\"indianred\">Note: ChatGPT's performance is changing all the time. To reproduce my output, use `chatgpt_verbs_300.csv`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab034812-8e9a-4a87-bb36-2febfe12b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_chatgpt = pd.read_csv(\"./output/chatgpt_verbs_300.csv\",index_col=0)[\"narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2eea1-4fa0-435a-9d6f-50721b171520",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparison of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38a0f3-1807-4f54-86fc-976bedd51830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(words):\n",
    "    df_word_freq = pd.DataFrame({\"Freq\":Counter(words)}).reset_index()\n",
    "    df_word_freq.columns = [\"Word\",\"Freq\"]\n",
    "    df_word_freq = df_word_freq.sort_values(\"Freq\",ascending=False)\n",
    "    return df_word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7e7be-9121-4a0e-838f-c7af0d327d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ret = pd.DataFrame()\n",
    "df_ret[\"Narrative\"] = df[\"narrative\"]\n",
    "df_ret[\"Morph\"] = verbs_morph2\n",
    "df_ret[\"ChatGPT\"] = verbs_chatgpt.values\n",
    "df_ret[\"ChatGPT\"] = df_ret[\"ChatGPT\"].map(lambda x:x.lower())\n",
    "df_ret = pd.concat([df_ret,df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c7911-80e5-4f6a-8f2c-2f37b445b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ret[~df_ret[\"ChatGPT\"].str.contains(\"fell|fall\")][[\"Narrative\",\"Morph\",\"ChatGPT\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69671d0a-078d-4564-9291-c40da9640f75",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cooccurrence Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3efe19-8c55-43b9-ae03-a43419e3d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 描画する単語数の限定\n",
    "def filter_word_by_frequency(texts, freq_top_n=0):\n",
    "    \"\"\" 元の書く文章の出現単語リストから上位N件の単語だけを残し返す。\n",
    "        N個の単語リストの各中身の要素を精査する。\n",
    "    \"\"\"\n",
    "    # Return the original texts\n",
    "    if freq_top_n <= 0:\n",
    "        return texts\n",
    "    \n",
    "    # Count word occurrences\n",
    "    c_words = Counter([word for text in texts for word in set(text)])\n",
    "    \n",
    "    # Get the top N words\n",
    "    top_words = [word for word,cnt in c_words.most_common(freq_top_n)]\n",
    "    \n",
    "    # Filter the original word lists to only include the top N words\n",
    "    texts_filtered = []\n",
    "    for text in texts:\n",
    "        filtered = list(set(top_words).intersection(set(text)))\n",
    "        if len(filtered) > 0:\n",
    "            texts_filtered.append(filtered)\n",
    "            \n",
    "    return texts_filtered\n",
    "\n",
    "# plotly config\n",
    "plotly_config = dict({\n",
    "    # 'scrollZoom':'cartesian',\n",
    "    'scrollZoom':True,\n",
    "    'displaylogo':False,\n",
    "    # 'modeBarButtonsToAdd':['drawopenpath',' drawrect','eraseshape\"],\n",
    "    'modeBarButtonsToRemove':[\"zoom\",\"select\",\" lasso2d\"],\n",
    "})\n",
    "\n",
    "# ノードサイズの正規化\n",
    "def norm_node_sizes(vals, adjuster=100):\n",
    "    \"\"\"Normalize the node sizes for plotting.\"\"\"\n",
    "    \n",
    "    v_max = max(vals)\n",
    "    normalized_vals = [val / v_max * adjuster for val in vals]\n",
    "    \n",
    "    return normalized_vals\n",
    "\n",
    "# エッジの計算\n",
    "def calc_jaccard_coef(texts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        texts (list): 各文章単位の単語リスト\n",
    "\n",
    "    Returns:\n",
    "        df_word_cnt (DataFrame): ジャッカード係数を含むデータフレーム\n",
    "    \"\"\"\n",
    "    \n",
    "    # 8語以上のテキストだけを保持\n",
    "    # texts = [text for text in texts if len(text)>8]\n",
    "    \n",
    "    # 各行における2語の組み合わせを生成\n",
    "    word_cmb_texts = [list(itertools.combinations(text, 2)) for text in texts]\n",
    "    \n",
    "    # 組み合わせの平坦化 (重複許す)\n",
    "    word_cmbs = sum(word_cmb_texts, [])\n",
    "    \n",
    "    # 単語Aと単語Bの積集合カウント\n",
    "    word_cmb_count = Counter(word_cmbs)\n",
    "    \n",
    "    # 単語Aと単語Bの積集合の算出\n",
    "    word_A = [k[0] for k in word_cmb_count.keys()]\n",
    "    word_B = [k[1] for k in word_cmb_count.keys()]\n",
    "    intersection_cnt = list(word_cmb_count.values())\n",
    "    \n",
    "    # データフレーム化\n",
    "    df_word_cnt = pd.DataFrame({\n",
    "        'WORD_A': word_A, \n",
    "        'WORD_B': word_B, \n",
    "        'ITS_CNT': intersection_cnt\n",
    "    })\n",
    "\n",
    "    # 単語Aの和集合の算出\n",
    "    word_A_cnt = df_word_cnt['WORD_A'].value_counts()\n",
    "    df_A = word_A_cnt.reset_index()\n",
    "    df_A.rename(columns={'count': 'WORD_A_CNT'}, inplace=True)\n",
    "\n",
    "    # 単語Bの和集合の算出\n",
    "    word_B_cnt = df_word_cnt['WORD_B'].value_counts()\n",
    "    df_B = word_B_cnt.reset_index()\n",
    "    df_B.rename(columns={'count': 'WORD_B_CNT'}, inplace=True)\n",
    "\n",
    "    # 左外部結合\n",
    "    df_word_cnt = pd.merge(df_word_cnt, df_A, how='left', on='WORD_A')\n",
    "    df_word_cnt = pd.merge(df_word_cnt, df_B, how='left', on='WORD_B')\n",
    "\n",
    "    # 同じ単語間の共起は除外\n",
    "    df_word_cnt = df_word_cnt[df_word_cnt[\"WORD_A\"] != df_word_cnt[\"WORD_B\"]]\n",
    "\n",
    "    # 単語A、単語Bの和集合カウント\n",
    "    df_word_cnt['UNION_CNT'] = df_word_cnt['WORD_A_CNT'] + df_word_cnt['WORD_B_CNT'] - df_word_cnt['ITS_CNT']\n",
    "\n",
    "    # Jaccard係数の算出\n",
    "    df_word_cnt['JACCARD'] = df_word_cnt['ITS_CNT'] / df_word_cnt['UNION_CNT']\n",
    "\n",
    "    return df_word_cnt\n",
    "\n",
    "# ネットワークの構築\n",
    "def build_interactive_network(G, pos, node_sizes, node_colors):\n",
    "      \n",
    "    # edgeデータの作成\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "    \n",
    "    # edgeデータの描画\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.5,color='#888'),\n",
    "        # hoverinfo='none',\n",
    "        opacity=1,\n",
    "        mode='lines')\n",
    "  \n",
    "    # nodeデータの作成\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "    \n",
    "    # ホバー時の表示テキスト作成\n",
    "    hover_texts = [f\"{word}:{val}\" for word,val in zip(list(G.nodes()),node_sizes)]\n",
    "  \n",
    "    # nodeの色、サイズ、マウスオーバーしたときに表示するテキストの設定\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        text=list(G.nodes()),\n",
    "        hovertext=hover_texts,#node_sizes,\n",
    "        textposition='top center',\n",
    "        mode='markers+text',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            colorscale='Portland',\n",
    "            reversescale=False,\n",
    "            color=node_colors,\n",
    "            size=norm_node_sizes(node_sizes), # size=node_sizes,\n",
    "            colorbar=dict(\n",
    "                thickness=16,\n",
    "                title='Word Appearance',\n",
    "            ),\n",
    "            line_width=2))\n",
    "     \n",
    "    data = [edge_trace, node_trace]\n",
    " \n",
    "    # レイアウトの設定\n",
    "    layout=go.Layout(\n",
    "                paper_bgcolor='rgba(0,0,0,0)',\n",
    "                plot_bgcolor='rgba(0,0,0,0)',\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                # margin=dict(b=10, l=5, r=5, t=10),\n",
    "                margin=dict(b=10, l=5, r=5, t=0),\n",
    "                font=dict(size=14),\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis = dict(showgrid = False, zeroline = False, showticklabels = False))\n",
    " \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.update_layout(\n",
    "        xaxis_title=None,\n",
    "        # title=dict(text=f\"頻出単語を表示\",font=dict(size=30)), \n",
    "        dragmode='pan',\n",
    "        margin=dict(b=0,l=0,r=0,t=20),\n",
    "        # title_pad=dict(b=0,l=10,r=0,t=15),\n",
    "        autosize=False, width=1000, height=500\n",
    "    )\n",
    "    # fig.show()\n",
    "    fig.show(config=plotly_config)\n",
    "    \n",
    "# 共起ネットワークの描画の準備とネットワーク図の取得\n",
    "def get_cooccurrence_network(texts, edge_threshold=20, freq_top_n=15):\n",
    "    \n",
    "    # 頻出topN単語にフィルタリング\n",
    "    texts = filter_word_by_frequency(texts, freq_top_n=freq_top_n)\n",
    "    # 単語リストの結合\n",
    "    words_list = sum(texts,[])\n",
    "    # 各単語の出現回数\n",
    "    word_count = Counter(words_list)\n",
    "\n",
    "    # 共起カウントとJaccard係数を算出\n",
    "    df_graph = calc_jaccard_coef(texts)\n",
    "\n",
    "    # 重みの分布を確認\n",
    "    # df_graph [\"ITS_CNT\"].describe()\n",
    "\n",
    "    # 単語ペア配列を取得\n",
    "    word_pairs = df_graph[[\"WORD_A\",\"WORD_B\"]].values\n",
    "\n",
    "    # 単語ぺア間の頻度を取得\n",
    "    edge_weights = df_graph[[\"ITS_CNT\"]].values.reshape(-1)\n",
    "    # edge_weights = df_graph[[“JACCARD”]] # 精度悪いので使用しない\n",
    "\n",
    "    # 単語ペアと重みの長さの確認\n",
    "    assert len(word_pairs)==len(edge_weights)\n",
    "\n",
    "    # ノード(単語)の作成\n",
    "    # ※ 抽出された単語が一文の中に1つしかない場合の単語はここでは除去されている\n",
    "    nodes = sorted(set(word_pairs.reshape(-1)))\n",
    "    \n",
    "    # グラフの作成\n",
    "    G = nx.Graph ()\n",
    "    # 接点／単語（node）の追加\n",
    "    G.add_nodes_from(nodes)\n",
    "    # print('Number of nodes: {}.format (G.number of nodes()))\n",
    "    \n",
    "    # 線（edge）の追加\n",
    "    # edge_thresholdで線を結ぶか判定\n",
    "    for i in range(len(word_pairs)):\n",
    "        pair = word_pairs [i]\n",
    "        edge_weight = edge_weights[i]\n",
    "        if edge_weight >= edge_threshold:\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    # print('Number of edges: (}'.format(G.number_of_edges()))\n",
    "    \n",
    "    # ノードが存在しない場合\n",
    "    if len(G.nodes())<=1:\n",
    "        return None\n",
    "                                    \n",
    "    # 孤立したnodeを削除\n",
    "    isolated = [n for n in G.nodes if len([i for i in nx.all_neighbors(G,n)]) == 0]\n",
    "    for n in isolated:\n",
    "        G.remove_node(n)\n",
    "    # print('Number of nodes: {}'.format(G.number_of_nodes()))\n",
    "\n",
    "    # 単語出現数でnodeサイズを変更する\n",
    "    # for n in G.nodes():\n",
    "    #     print(n)\n",
    "    word_size_dic = {node:word_count[node] for node in G.nodes()}\n",
    "\n",
    "    # 名文章で出てくる割合\n",
    "    # 既に単語は1文1カウントになっている\n",
    "    n_data = len(texts)\n",
    "    word_freq_dic = {key:val/n_data for key, val in word_size_dic.items()}\n",
    "    \n",
    "    # グラフレイアウト\n",
    "    pos = nx.spring_layout(G,k=None) # k = node間反発係数\n",
    "    \n",
    "    # nodeの色をページランクアルゴリズムによる重要度により変える\n",
    "    # pr = nx_pagerank(G)\n",
    "    \n",
    "    # print(word_size_dic)\n",
    "    \n",
    "    # インタラクティブな共起ネットワークの可視化\n",
    "    fig = build_interactive_network (G, pos, list(word_size_dic.values()), list(word_freq_dic.values()))\n",
    "    # fig = build_Cooccurrence_network(G, pos, list(word_size_dic.values()), list(pr.values()))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83946235-0e78-435d-8ebd-9616a861e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each word list of narratives\n",
    "lines = df_ret['Morph'].values\n",
    "get_cooccurrence_network(lines, edge_threshold=3, freq_top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31823796-f2f0-4e6c-b6f8-a5edb98770b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Get each word list of narratives\n",
    "# Remove unnecessary words from narrative\n",
    "remove_pos_tag_list = ['CC', 'JJ', ',', '.', ')', '(', 'IN']\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += [\"sustained\",\"was\",\"fx\",\"wa\"]\n",
    "\n",
    "def remove_words_from_narrative(s):\n",
    "    s = s.lower()\n",
    "    ws = nltk.word_tokenize(s)\n",
    "    ws = [w for w in ws if w.isalpha()]\n",
    "    pos = nltk.pos_tag(ws)\n",
    "    ws = [p[0] for p in pos if (not p[1] in remove_pos_tag_list)&(not p[0] in stop_words)]\n",
    "    return ws\n",
    "\n",
    "# lines = df_ret['ChatGPT'].map(word_tokenize)\n",
    "df_ret['ChatGPT_refined'] = df_ret['ChatGPT'].map(remove_words_from_narrative)\n",
    "lines = df_ret['ChatGPT_refined'].values\n",
    "get_cooccurrence_network(lines, edge_threshold=3, freq_top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc8a46-2441-4a07-85e3-abb1851ee4c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Differences in antecedent events among demographic groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736131d-af76-466e-b437-b9cfaac67f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret[\"age_c\"] = df_ret[\"age\"].map(lambda x:x//5 *5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644d297-b617-403c-897a-590338c276f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_col='age_c' #'race'#'age_c','sex'\n",
    "\n",
    "# 空の辞書を作成して、カテゴリごとに単語の頻出リストを格納\n",
    "word_freq = {}\n",
    "for category in sorted(df_ret[category_col].unique()):\n",
    "    # そのカテゴリのすべての単語を1つのリストに結合\n",
    "    words_in_category = df_ret[df_ret[category_col] == category]['ChatGPT_refined'].sum()\n",
    "    # 単語の出現回数をカウント\n",
    "    word_count = pd.Series(words_in_category).value_counts().to_dict()\n",
    "    word_freq[category] = word_count\n",
    "    \n",
    "pd.DataFrame(word_freq).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483f441-9398-4b01-b07c-298cd9f0bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret[\"age_c\"].value_counts().sort_index()\n",
    "# 186/114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a9d2d-73e6-4dd9-88de-c6e1e7b7606e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
